{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QLKZ77x4v_-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f648c7ba-4d75-410f-804a-6b03962d1f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "Targets:\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ],
      "source": [
        "# Input data (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Target values (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')\n",
        "\n",
        "# Convert numpy data to tensors\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "print('Inputs:', inputs, sep='\\n')\n",
        "print('Targets:', targets, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases\n",
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1,3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "print('w:', w, sep='\\n')\n",
        "print('b:', b, sep='\\n')"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdbb60b0-6721-4f53-f584-435a5c932fc9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w:\n",
            "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "b:\n",
            "tensor([0.6213], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sigmoid function\n",
        "def model(x, w, b):\n",
        "  pred = 1/(1+torch.exp(-(x @ w.t() + b)))\n",
        "  return pred\n",
        "\n",
        "pred = model(inputs, w, b)\n",
        "print('Predictions with initial weights and biases (probabilities):', pred, sep='\\n')"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e32c0ff-d56f-4564-f4e9-16862bfd9c91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions with initial weights and biases (probabilities):\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define logistic regression loss function (binary crosss-entropy)\n",
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "  eps = 1e-9\n",
        "  loss = -torch.mean(true_labels*torch.log(predicted_probs+eps) + (1-true_labels)*torch.log(1-predicted_probs+eps))\n",
        "  return loss\n",
        "\n",
        "loss = binary_cross_entropy(pred, targets)\n",
        "print('Loss function:', loss, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWmkTN6GY6k5",
        "outputId": "7180803e-8f90-40d9-dc8f-216c8c9a2821"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function:\n",
            "tensor(8.2893, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Back propagation\n",
        "loss.backward()\n",
        "\n",
        "print('Weights gradient:', w.grad, sep='\\n')\n",
        "print('Bias gradient:', b.grad, sep='\\n')"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b203e3-0fb3-483f-b7e7-5a284cdbbaa2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights gradient:\n",
            "tensor([[1.0201e-20, 9.3628e-21, 6.0090e-21]])\n",
            "Bias gradient:\n",
            "tensor([1.3974e-22])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize small weights and biases values to prevent gradient vanishing problem\n",
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000\n",
        "\n",
        "# Calculate predictions, loss function and gradients with new gradient values\n",
        "pred = model(inputs, w, b)\n",
        "loss = binary_cross_entropy(pred, targets)\n",
        "loss.backward()\n",
        "print('w:', w, sep='\\n')\n",
        "print('b:', b, sep='\\n')\n",
        "print('Loss function:', loss, sep='\\n')\n",
        "print('Predictions with initial weights and biases:', pred, sep='\\n')\n",
        "print('Weights gradient:', w.grad, sep='\\n')\n",
        "print('Bias gradient:', b.grad, sep='\\n')"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805a9c94-bc79-4647-973a-8311996d5f88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w:\n",
            "tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True)\n",
            "b:\n",
            "tensor([0.0006], requires_grad=True)\n",
            "Loss function:\n",
            "tensor(0.6829, grad_fn=<NegBackward0>)\n",
            "Predictions with initial weights and biases:\n",
            "tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n",
            "Weights gradient:\n",
            "tensor([[ -5.4417, -18.9853, -10.0682]])\n",
            "Bias gradient:\n",
            "tensor([-0.0794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define gradient descent function\n",
        "def gradient_descent(epochs, lr, x, y, w, b):\n",
        "\n",
        "  for e in range(epochs):\n",
        "\n",
        "    pred = model(x, w, b)\n",
        "    loss = binary_cross_entropy(pred, y)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "      w -= w.grad * lr\n",
        "      b -= b.grad * lr\n",
        "      w.grad.zero_()\n",
        "      b.grad.zero_()\n",
        "\n",
        "  return w,b"
      ],
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate predictions (probabilities and classes)\n",
        "threshold=0.5\n",
        "w,b = gradient_descent(1000, 1e-5, inputs, targets, w, b)\n",
        "pred = model(inputs, w, b)\n",
        "print(f'Predicted class 1 probabilities: \\n{pred}')\n",
        "pred_class = (pred>=threshold).float()\n",
        "print(f'Predicted classes with threshold={threshold}: \\n{pred_class}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noSb-hFOMtIU",
        "outputId": "694a88b1-9666-410b-b468-f6dc8b039a25"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class 1 probabilities: \n",
            "tensor([[0.5777],\n",
            "        [0.6685],\n",
            "        [0.9113],\n",
            "        [0.1616],\n",
            "        [0.8653]], grad_fn=<MulBackward0>)\n",
            "Predicted classes with threshold=0.5: \n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tensor dataset & data loader\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "mBFIRBssOAqE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48750c5-a49b-451e-f358-de1cad208add"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6247b275-2117-42d8-f76b-2dd38ad8a8b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.],\n",
              "         [102.,  43.,  37.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogReg(nn.Module):\n",
        "\n",
        "  #Initialize the layers\n",
        "  def __init__(self, ):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(3,1) # Linear layer with 3 inputs and 1 output\n",
        "    self.layer2 = nn.Sigmoid() # Activation function\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = LogReg()"
      ],
      "metadata": {
        "id": "d8ibQfBS3LA5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.SGD(model_2.parameters(), lr=1e-5)\n",
        "loss = F.binary_cross_entropy"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model(inputs, w, b)\n",
        "print('Predictions based on current model parameter values (class 1 probabilities):\\n', pred)\n",
        "pred_class = (pred>=threshold).float()\n",
        "print(f'Predicted classes with threshold={threshold}: \\n{pred_class}')\n",
        "print('Evaluating the loss function using the current model parameters: \\n', loss(pred, targets))\n",
        "print('F1 score: \\n', f1_score(targets, pred_class))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTD0b9SSNFcU",
        "outputId": "9050ba28-9aa0-4f7c-a111-b5956d0d3a1f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions based on current model parameter values (class 1 probabilities):\n",
            " tensor([[0.5777],\n",
            "        [0.6685],\n",
            "        [0.9113],\n",
            "        [0.1616],\n",
            "        [0.8653],\n",
            "        [0.5777],\n",
            "        [0.6685],\n",
            "        [0.9113],\n",
            "        [0.1616],\n",
            "        [0.8653],\n",
            "        [0.5777],\n",
            "        [0.6685],\n",
            "        [0.9113],\n",
            "        [0.1616],\n",
            "        [0.8653]], grad_fn=<MulBackward0>)\n",
            "Predicted classes with threshold=0.5: \n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Evaluating the loss function using the current model parameters: \n",
            " tensor(0.3357, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "F1 score: \n",
            " 0.8571428571428571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets == pred_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNonCRqzToSj",
        "outputId": "fc74436b-7ebf-431a-92cc-f328387313ba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [False],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [ True],\n",
              "        [ True]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Висновки**\n",
        "Модель добре навчилась, про що свідчить наступне:\n",
        "1. Низьке значення Loss function (0.3357);\n",
        "1. f1 score = 0.857143;\n",
        "1. У наборі даних з 20 елементів модель помилилася у передбаченні трічі."
      ],
      "metadata": {
        "id": "imfAXLooTg0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "1g26MNEQ0-M9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = fit_return_loss(1000, model_2, loss, opt, train_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0-vj_CR_o4H",
        "outputId": "18e97f2a-0b1a-48e4-fee3-fdfa72cfcad7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 6.6400\n",
            "Epoch [20/1000], Loss: 6.0483\n",
            "Epoch [30/1000], Loss: 5.7263\n",
            "Epoch [40/1000], Loss: 5.4710\n",
            "Epoch [50/1000], Loss: 5.2474\n",
            "Epoch [60/1000], Loss: 5.0653\n",
            "Epoch [70/1000], Loss: 4.8974\n",
            "Epoch [80/1000], Loss: 4.7861\n",
            "Epoch [90/1000], Loss: 4.6903\n",
            "Epoch [100/1000], Loss: 4.5353\n",
            "Epoch [110/1000], Loss: 4.4753\n",
            "Epoch [120/1000], Loss: 4.3686\n",
            "Epoch [130/1000], Loss: 4.2792\n",
            "Epoch [140/1000], Loss: 4.2018\n",
            "Epoch [150/1000], Loss: 4.1132\n",
            "Epoch [160/1000], Loss: 4.0389\n",
            "Epoch [170/1000], Loss: 3.9423\n",
            "Epoch [180/1000], Loss: 3.8701\n",
            "Epoch [190/1000], Loss: 3.7756\n",
            "Epoch [200/1000], Loss: 3.6974\n",
            "Epoch [210/1000], Loss: 3.6101\n",
            "Epoch [220/1000], Loss: 3.5297\n",
            "Epoch [230/1000], Loss: 3.4427\n",
            "Epoch [240/1000], Loss: 3.3760\n",
            "Epoch [250/1000], Loss: 3.2822\n",
            "Epoch [260/1000], Loss: 3.1985\n",
            "Epoch [270/1000], Loss: 3.1111\n",
            "Epoch [280/1000], Loss: 3.0308\n",
            "Epoch [290/1000], Loss: 2.9483\n",
            "Epoch [300/1000], Loss: 2.8748\n",
            "Epoch [310/1000], Loss: 2.7879\n",
            "Epoch [320/1000], Loss: 2.7048\n",
            "Epoch [330/1000], Loss: 2.6292\n",
            "Epoch [340/1000], Loss: 2.5456\n",
            "Epoch [350/1000], Loss: 2.4710\n",
            "Epoch [360/1000], Loss: 2.4122\n",
            "Epoch [370/1000], Loss: 2.3102\n",
            "Epoch [380/1000], Loss: 2.2363\n",
            "Epoch [390/1000], Loss: 2.1587\n",
            "Epoch [400/1000], Loss: 2.0854\n",
            "Epoch [410/1000], Loss: 2.0062\n",
            "Epoch [420/1000], Loss: 1.9342\n",
            "Epoch [430/1000], Loss: 1.8632\n",
            "Epoch [440/1000], Loss: 1.7922\n",
            "Epoch [450/1000], Loss: 1.7232\n",
            "Epoch [460/1000], Loss: 1.6485\n",
            "Epoch [470/1000], Loss: 1.5853\n",
            "Epoch [480/1000], Loss: 1.5150\n",
            "Epoch [490/1000], Loss: 1.4520\n",
            "Epoch [500/1000], Loss: 1.3831\n",
            "Epoch [510/1000], Loss: 1.3171\n",
            "Epoch [520/1000], Loss: 1.2595\n",
            "Epoch [530/1000], Loss: 1.2084\n",
            "Epoch [540/1000], Loss: 1.1388\n",
            "Epoch [550/1000], Loss: 1.0851\n",
            "Epoch [560/1000], Loss: 1.0415\n",
            "Epoch [570/1000], Loss: 0.9843\n",
            "Epoch [580/1000], Loss: 0.9393\n",
            "Epoch [590/1000], Loss: 0.8865\n",
            "Epoch [600/1000], Loss: 0.8436\n",
            "Epoch [610/1000], Loss: 0.8030\n",
            "Epoch [620/1000], Loss: 0.7632\n",
            "Epoch [630/1000], Loss: 0.7308\n",
            "Epoch [640/1000], Loss: 0.7033\n",
            "Epoch [650/1000], Loss: 0.6679\n",
            "Epoch [660/1000], Loss: 0.6437\n",
            "Epoch [670/1000], Loss: 0.6134\n",
            "Epoch [680/1000], Loss: 0.5918\n",
            "Epoch [690/1000], Loss: 0.5703\n",
            "Epoch [700/1000], Loss: 0.5492\n",
            "Epoch [710/1000], Loss: 0.5325\n",
            "Epoch [720/1000], Loss: 0.5149\n",
            "Epoch [730/1000], Loss: 0.5012\n",
            "Epoch [740/1000], Loss: 0.4870\n",
            "Epoch [750/1000], Loss: 0.4743\n",
            "Epoch [760/1000], Loss: 0.4654\n",
            "Epoch [770/1000], Loss: 0.4552\n",
            "Epoch [780/1000], Loss: 0.4421\n",
            "Epoch [790/1000], Loss: 0.4315\n",
            "Epoch [800/1000], Loss: 0.4263\n",
            "Epoch [810/1000], Loss: 0.4149\n",
            "Epoch [820/1000], Loss: 0.4085\n",
            "Epoch [830/1000], Loss: 0.4013\n",
            "Epoch [840/1000], Loss: 0.3940\n",
            "Epoch [850/1000], Loss: 0.3883\n",
            "Epoch [860/1000], Loss: 0.3829\n",
            "Epoch [870/1000], Loss: 0.3770\n",
            "Epoch [880/1000], Loss: 0.3719\n",
            "Epoch [890/1000], Loss: 0.3672\n",
            "Epoch [900/1000], Loss: 0.3627\n",
            "Epoch [910/1000], Loss: 0.3587\n",
            "Epoch [920/1000], Loss: 0.3549\n",
            "Epoch [930/1000], Loss: 0.3522\n",
            "Epoch [940/1000], Loss: 0.3474\n",
            "Epoch [950/1000], Loss: 0.3445\n",
            "Epoch [960/1000], Loss: 0.3411\n",
            "Epoch [970/1000], Loss: 0.3395\n",
            "Epoch [980/1000], Loss: 0.3350\n",
            "Epoch [990/1000], Loss: 0.3319\n",
            "Epoch [1000/1000], Loss: 0.3297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.xticks(np.arange(0,1001, step=100))\n",
        "\n",
        "plt.title('Evaluated losses for current epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "yfuff96o_zU2",
        "outputId": "18046050-6d5a-448d-adae-52657199ce5b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model_2(inputs)\n",
        "print(f'Predicted class 1 probabilities: \\n{pred}')\n",
        "pred_class_final = (pred>=threshold).float()\n",
        "print(f'Predicted classes with threshold={threshold}: \\n{pred_class_final}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNNV7r2GL4Za",
        "outputId": "c33e0049-0b8b-47dc-87d7-78b3b5703743"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class 1 probabilities: \n",
            "tensor([[0.5729],\n",
            "        [0.6791],\n",
            "        [0.9026],\n",
            "        [0.1569],\n",
            "        [0.8761],\n",
            "        [0.5729],\n",
            "        [0.6791],\n",
            "        [0.9026],\n",
            "        [0.1569],\n",
            "        [0.8761],\n",
            "        [0.5729],\n",
            "        [0.6791],\n",
            "        [0.9026],\n",
            "        [0.1569],\n",
            "        [0.8761]], grad_fn=<SigmoidBackward0>)\n",
            "Predicted classes with threshold=0.5: \n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(targets, pred_class_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVo8OkFWNhsN",
        "outputId": "3dec4a5a-eaa0-4288-ff38-3340dbc37026"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8571428571428571"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss(pred, targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9_PgA6HVeJ7",
        "outputId": "93aeab31-0013-4ec3-8632-f9fe49e8f450"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.3286, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_class == pred_class_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzEN3dSuT8oD",
        "outputId": "e8c10b9f-8e30-4fe3-9c1a-caa9968e9656"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Висновки**\n",
        "\n",
        "Loss function другої моделі ще нижче (0.3286), ніж у першої. Це свідчить, що друга модель краща,але несуттєво (loss function першої моделі складала 0.3357).\n",
        "Мітки передбачень та f1_score для моделей однакові.\n",
        "f1 score = 0.857143 свідчить про те, що модель добре генералізує."
      ],
      "metadata": {
        "id": "uRkspwNxVLjS"
      }
    }
  ]
}